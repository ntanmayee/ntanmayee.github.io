<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Algorithms for Causal Discovery</title>
  <meta name="description" content="This article considers the problem of estimating the structure of the causal DAG given some observational data.">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-127309835-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-127309835-1');
  </script>


  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  

  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  <link rel="stylesheet" type="text/css" href="/css/tufte.css">
  <link rel="stylesheet" type="text/css" href="/css/latex.css">
  <!-- <link rel="stylesheet" type="text/css" href="/css/print.css" media="print"> -->

  <link rel="canonical" href="/articles/2017/02/07/causal-discovery.html">

  <link rel="alternate" type="application/rss+xml" title="Tanmayee Narendra" href="/feed.xml" />
  <meta name="google-site-verification" content="HF7Ny1t4g_5j_isSGuyH1QjH64CHcrEuNPcPY3OM5UQ" />
</head>

  
  <body>
    <!--- Header and nav template site-wide -->
<header>
	
	<nav class="group">
	
	
		
		
		
		
		
		
		
				
					<a href="/">Tanmayee Narendra</a>
				
			
		
		
				
					<a href="/blog/">Blog</a>
				
			
		
		
				
					<a href="/page/publications">Publications</a>
				
			
		
	</nav>
</header>

    <article class="group">
      <h1>Algorithms for Causal Discovery</h1>
<p class="subtitle">February 7, 2017</p>

<p>This article considers the problem of estimating the structure of the causal DAG given some observational data. <!--more--></p>

<blockquote>
  <p>How difficult is it to learn a DAG?</p>
</blockquote>

<p>The number of DAGs for $n$ nodes increases super exponentially in $n$.</p>

<script type="math/tex; mode=display">0\ 1 \\1\ 1\\2\ 3\\3\ 25\\4\ 543\\5\ 29281\\6\ 3781503\\7\ 1138779265\\ 8\ 783702329343\\ 9\ 1213442454842881\\ 10\ 4175098976430598143\\</script>

<p>The entire list upto $n=40$ can be found <a href="https://oeis.org/A003024/b003024.txt">here.</a></p>

<p>From what I have read so far, there are three major methods to learn causal structures. They are</p>

<ol>
  <li>Methods based on Independence Tests</li>
  <li>Score based methods</li>
  <li>Methods based on making assumptions about the distribution</li>
</ol>

<p>The first two methods estimate the true causal DAG upto the Markov equivalent class. What that means is that the final DAG that is output from the algorithm may contain undirected edges (formally called CPDAG - completed partially directed acyclic graph). Algorithms in the third method claim to estimate the true causal DAG, assuming that the required conditions are met.</p>

<p>The following is an overview of the algorithms in each method.</p>

<h3 id="methods-based-on-independence-tests">Methods based on Independence Tests</h3>
<p>Consider three variables $A$, $B$ and $C$. Let us assume that there are no unobserved variables. These variables can be arranged in four ways.
<img src="https://raw.githubusercontent.com/triptoes1/triptoes1.github.io/master/assets/images/v-structure.png" alt="dag structures" /></p>

<p>Figures 1,2 and 3 represent the same set of conditional independence relationships between the three variables, and Figure 4 represents a different set of conditional independence relationships.</p>

<p>Let us consider Figure 4, with $A$ and $C$ both pointing towards $B$ (This is sometimes called a collider). $A$ and $C$ are dependent conditioned on $B$. One way to understand this is to consider $A$ and $C$ as the outcomes of coin tosses of two separate coins, and $C$ as a random variable that takes the value of $1$ if both coins give the same outcome, and $0$ otherwise. The two coin tosses are independent unconditionally. But, once we know the value of $B$, $A$ and $C$ are dependent on each other.</p>

<p>Algorithms based on Independence Tests primarily use this property of colliders to identify $v$ structures from the data. However, they cannot orient all edges because conditional independence testing cannot distinguish between Figures (1), (2) and (3). Hence, these algorithms can estimate the causal DAG upto the Markov equivalence class.</p>

<h3 id="score-based-methods">Score Based Methods</h3>
<p>In score based methods, we assign a score to every DAG depending on the goodness of fit with the data, and search over the space of DAGs to find the one with the best score. The number of DAGs for $n$ variables grows super-exponentially, as seen earlier.</p>

<p>Greedy search algorithms are used to find the best scoring DAG.</p>

<h3 id="other-methods">Other Methods</h3>
<p>If we consider the framework of Structural Equation Models (also Structural Causal Models), making some assumptions about the model helps us recover the true DAG.</p>

<p>Let us formally define Structural Equation Models (SEM). For some set of random variables $X={X_1,..,X_p}$, the general SEM is defined as a set of functions
<script type="math/tex">X_i = f_i(PA_i, N_i)</script>
where $N_1,..,N_p$ are mutually independent. $PA_i$ denotes the parents of $X_i$ in the DAG, $N_i$ is the noise variable and $f_i$ is a function from $\mathbb{R}^{|PA_i|+1} \rightarrow \mathbb{R}$. Depending on the kind of assumption made on $f_i$, there are three different approaches that I have come across.</p>

<h4 id="linear-gaussian-acyclic-model-lingam">Linear Gaussian Acyclic Model (LinGAM)</h4>
<p>Consider a linear SEM
<script type="math/tex">X_j = \sum\limits_{k \in PA_j} \beta_{jk} X_k + N_j</script>
where all $N_j$ are mutually independent and are non-Gaussian, and $\beta_{jk} \ne 0$ for all $k \in PA_j$. This SEM is also called as LinGAM.</p>

<p>Basically, this model makes the assumption that every variable in the SEM depends linearly on its parents. In 2006, it was proved that the original DAG is identifiable from this SEM. A practical method for finite data uses Independent Component Analysis (ICA) to estimate the true DAG. The original paper can be found <a href="http://www.jmlr.org/papers/volume7/shimizu06a/shimizu06a.pdf">here</a></p>

<h4 id="causal-additive-models-cam">Causal Additive Models (CAM)</h4>
<p>Consider an SEM of the form
<script type="math/tex">X_j = \sum \limits_{k \in PA_j} f_{j,k}(X_k) + N_j</script>
with $N_1, â€¦, N_p$ mutually independent with $N_j=\mathcal{N}(0, \sigma_j^2)$, $\sigma_j^2 &gt; 0$ for all $j$. Also, all $f_{j,k}(.)$ are smooth functions from $\mathbb{R} \rightarrow \mathbb{R}$.</p>

<p>In other words, we assume that each variable is determined by a sum of functions of its parents with a Gaussian noise component. It has been proved that if the functions $f_{j,k}$ are non-linear, the DAG is identifiable from the joint distribution. <a href="https://arxiv.org/pdf/1310.1533.pdf">This paper</a> provides a broad framework to identify the DAG from finite data, assuming the distribution meets the conditions mentioned earlier. This method is feasible for upto 200 variables.</p>

<h4 id="additive-noise-models-anm">Additive Noise Models (ANM)</h4>
<p>Consider the Additive Noise Model (ANM) which is of the form
<script type="math/tex">X_j = f_j(PA_j) + N_j</script>
where $PA_j$ denotes the parents of $X_J$, functions $f_j$ are non-constant and all noise variables $N_j$ are mutually independent. If we additionally assume that the noise variables have non-vanishing densities and the functions $f_j$ are three times continuously differentiable, it has been proved that the original DAG is identifiable from the joint distribution. One practical method for finite data is Regression with Subsequent Independence Test (RESIT). More about the method can be found <a href="http://jmlr.org/papers/volume15/peters14a/peters14a.pdf">here.</a> ANM is more general than CAM. This method is feasible for upto 20 variables.</p>





  <div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = '/articles/2017/02/07/causal-discovery.html';
      this.page.identifier = '/articles/2017/02/07/causal-discovery.html';
    };
    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://triptoes1.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>



    </article>
    <span class="print-footer">Algorithms for Causal Discovery - February 7, 2017 - Tanmayee Narendra</span>
    <footer>
  <hr class="slender">
<div class="credits">
<span>&copy; 2020 
  
		<a href="mailto:ntanmayee [at] gmail [dot] com">Tanmayee Narendra</a></span></br> <br>    
    

<span>Created with <a href="//jekyllrb.com">Jekyll</a> and the theme <a href="//github.com/sdruskat/tufte-css-jekyll">tufte-css-jekyll</a>.</span> 
</div>  
</footer>
  </body>
</html>
